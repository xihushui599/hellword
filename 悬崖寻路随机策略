import gym
import numpy as np
env=gym.make('CliffWalking-v0')
print('观察空间={}'.format(env.observation_space))
print('动作空间={}'.format(env.action_space))
print('状态数量={},动作数量={}'.format(env.nS,env.nA))
print('地图大小={}'.format(env.shape))#shape就是求解数组的维数
def play_once(env,policy):
    total_reward=0
    state=env.reset()#初始状态36
    loc = np.unravel_index(state,env.shape)#索引state在数组中的位置
    print('状态={},位置={}'.format(state,loc))#初始位置（3，0）
    while True:
        action=np.random.choice(env.nA,p=policy[state])#按照指定的概率分布选择一个动作,动作合集（0，1，2，3），p是一个列表
        next_state,reward,done ,_ = env.step(action)
        print('状态={}，位置={}，奖励={}'.format(next_state,loc,reward))
        total_reward+=reward
        if done:
            break
        state=next_state
    return total_reward
optimal_policy=np.ones((48,4),dtype=int)/4#等概率的策略
total_reward=play_once(env,optimal_policy)
print('总奖励={}'.format(total_reward))

def evaluate_bellman(env,policy,gamma=1):
    a,b = np.eye(env.nS),np.zeros((env.nS))
    for state in range(env.nS-1):
        for action in range(env.nA):
            pi = policy[state][action]
            for p,next_state,reward,done in env.P[state][action]:#系统的一个反馈
                a[state,next_state]-=(pi*gamma)
                b[state]+=(pi*reward*p)
    v = np.linalg.solve(a,b.reshape(-1,1))
    q = np.zeros((env.nS,env.nA))
    for state in range(env.nS-1):
        for action in range(env.nA):
            for p,next_state,reward,done in env.P[state][action]:#系统的一个反馈
                q[state,action]+=(p*(reward+gamma*v[next_state]))   
    return v,q
policy = np.random.uniform(size=(env.nS,env.nA))
policy = policy / np.sum(policy,axis=1)[:,np.newaxis]
state_valies,action_values = evaluate_bellman(env,policy)
print('状态价值={}'.format(state_valies))
print('动作价值={}'.format(action_values))
