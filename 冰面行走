import gym
import numpy as np
env=gym.make('FrozenLake-v0')
env=env.unwrapped
print(env.observation_space)
print(env.action_space)
print(env.P[14][2])
def play_policy(env,policy,render=False):
    total_reward=0
    observation=env.reset()
    while True:
        if render:
            env.render()
        action=np.random.choice(env.action_space.n,p=policy[observation])
        observation,reward,done,_=env.step(action)
        total_reward+=reward
        if done:
            break
    return total_reward
random_policy=np.ones((env.unwrapped.nS,env.unwrapped.nA))/env.unwrapped.nA
episode_rewards=[play_policy(env,random_policy) for _ in range(100)]
print("随机策略 平均奖励:{}".format(np.mean(episode_rewards)))

def v2q(env,v,s=None,gamma=1):
    if s is not None:
        q=np.zeros(env.unwrapped.nA)
        for a in range(env.unwrapped.nA):
            for prob,next_state,reward,done in env.unwrapped.P[s][a]:
                q[a]+=prob*(reward+gamma*v[next_state]*(1-done))#转移到终止状态的时候，状态的价值为0
    else:
        q=np.zeros((env.unwrapped.nS,env.unwrapped.nA))
        for s in range(env.unwrapped.nS):
            q[s]=v2q(env,v,s,gamma)
    return q #这一段程序设计的特别好，自我迭代
def evaluate_policy(env,policy,gamma=1,tolerant=1e-6):
    v=np.zeros(env.unwrapped.nS)
    while True:
        delta=0
        for s in range(env.unwrapped.nS):
            vs=np.sum(policy[s]*v2q(env,v,s,gamma))
            delta=max(delta,abs(v[s]-vs))
            v[s]=vs
        if delta<tolerant:
            break
    return v
print('状态价值函数：')
v_random=evaluate_policy(env,random_policy)
print(v_random.reshape(4,4))
print('动作价值函数：')
q_random=v2q(env,v_random)
print(q_random)
def improve_policy(env,v,policy,gamma=1):
    optimal=True
    for s in range(env.unwrapped.nS):
        q=v2q(env,v,s,gamma)
        a=np.argmax(q)
        if policy[s][a]!=1.0:
            optimal=False
            policy[s]=0.0
            policy[s][a]=1.0
    return optimal
policy = random_policy.copy()#随机策略的拷贝
optimal=improve_policy(env,v_random,policy)
if optimal:
    print('无更新,最优策略:')
else:
    print('有更新，更新后的策略为：')
print(policy)
def iterate_policy(env,gamma=1.0,tolerant=1e-6):
    policy = np.ones((env.unwrapped.nS,env.unwrapped.nA))/env.unwrapped.nA
    while True:
        v=evaluate_policy(env,policy,gamma,tolerant)
        if improve_policy(env,v,policy):
            break
    return policy,v
policy_pi,v_pi=iterate_policy(env)
print('状态价值函数=')
print(v_pi.reshape(4,4))
print('最优策略=')
print(np.argmax(policy_pi,axis=1).reshape(4,4))
