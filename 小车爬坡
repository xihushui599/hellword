#while True:
#    positions.append(observation[0])
#    velocities.append(observation[1])
#    next_observation,reward,done,_ = env.step(2)
#    if done:
#        break
#    observation = next_observation
#    if len(positions) > 1000:
#        break
#if next_observation[0] > 0.5:
#    print('成功到达')
#else:
#    print('失败退出')
#fig,ax = plt.subplots()
#ax.plot(positions,label='position')
#ax.plot(velocities,label='velocity')
#ax.legend
# plt.show()
#class TileCoder:
#    def __init__(self, layers, features):
#        self.layers = layers
#        self.features = features
#        self.codebook = {}
#    def get_feature(self,codeword):
#        if codeword in self.codebook:
#            return self.codebook[codeword]
#        count = len(self.codebook)
#        if count >= self.features:
#            return hash(codeword)%self.features
#        self.codebook[codeword] = count
#        return count
#    def __call__(self,floats=(),ints=()):
#        dim = len(floats)
#        scaled_floats = tuple(f*self.layers*self.layers for f in floats)
#        features = []
#        for layer in range(self.layers):
#            codeword = (layer,)+tuple(int((f+(1+dim*i)*layer)/self.layers)\
#                for i,f in enumerate(scaled_floats))+ints
#            feature = self.get_feature(codeword)
#            features.append(feature)
#        return features
#class SARSAAgent:
#    def  __init__(self,env,layers=8,features=1893,gamma=1.,learning_rate=0.03,epsilon=0.001):
#        self.action_n = env.action_space.n
#        self.obs_low=env.observation_space.low
#        self.obs_scale = env.observation_space.high-\
#            env.observation_space.low
#        self.encoder = TileCoder(layers,features)
#        self.w=np.zeros(features)#权重
#        self.gamma=gamma
#        self.learning_rate = learning_rate
#        self.epsilon = epsilon
#    def encode(self,observation,action):
#        states = tuple((observation-self.obs_low)/self.obs_scale)
#        actions = (action,)
#        return self.encoder(states,actions)
#    def get_q(self,observation,action):
#        features = self.encode(observation,action)
#        return self.w[features],sum()
#    def decide(self,observation):
#        if np.random.rand() < self.epsilon:#rand是生成0-1之间的均匀数不包括1
#            return np.random.randint(self.action_n)
#        else:
#            qs = [self.get_q(observation,action) for action in range(self.action_n)]
#            return np.argmax(qs)
#    def learn(self,observation,action,reward,next_observation,done,next_action):
#        u = reward+(1.-done)*self.gamma*self.get_q(next_observation,next_action)
#        td_error = u-self.get_q(observation,action)
#        features = self.encode(observation,action)
#        self.w[features] +=(self.learning_rate*td_error)
#def play_sarsa(env,agent,train=False,render=False):
#    episode_reward = 0
#    observation = env.reset()#生成初始状态,这个状态实际上是一个矩阵的行数，与实际的位置之间有一个编码
#    action = agent.decide(observation)#生成初始动作
#    while True:
#        if render:
#            env.render()
#        next_observation,reward,done,_ = env.step(action)
#        episode_reward+=reward
#        next_action = agent.decide(next_observation)
#        if train:
#            agent.learn(observation,action,reward,next_observation,done,next_action)
#        if done:
#            break
#        observation,action = next_observation,next_action#进一步迭代循环动作价值
#    return episode_reward
